<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>我的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="我的博客">
<meta property="og:url" content="/index.html">
<meta property="og:site_name" content="我的博客">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="我的博客">
  
  
<!--    <link rel="icon" href="/favicon.ico">-->
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value=""></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">我的博客</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-java/java-nio" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/15/java/java-nio/" class="article-date">
  <time datetime="2019-07-15T06:49:35.000Z" itemprop="datePublished">2019-07-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/15/java/java-nio/">java-nio-server</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>之前对Java nio的了解局限于简答的socket数据传输，对tcp沾包、拆包有一些简单的了解，但是没有深入的研究。后来<br>简单的了解了一些tcp知识，例如：tcp滑动窗口。</p>
<p>最近粗略的看了一下zookeeper的源码，了解了zookeeper的底层实现，比如zk节点保存结构是使用的DataTree,临时节点使用的是Map&lt;String sessionId,Set<string path>&gt;进行保存的，使用ExpireQueue进行更行节点的过期时间（sessionId过期）。</string></p>
<p>zk中实现了基于java nio和netty两种socket编程，本文根据zk Java nio实现socket服务端编程。测试时，客户端使用的是telnet进来连接。</p>
<p>java nio中比较核心的几个类:</p>
<pre><code>java.nio.channels.ServerSocketChannel
java.nio.channels.SocketChannel
java.nio.channels.SelectionKey
java.nio.channels.Selector</code></pre><p>以下是核心代码，相关引用代码请clone zookeeper代码进行查看学习</p>
<pre><code>package server;

import common.ExpiryQueue;
import common.WorkerService;
import common.ZooKeeperThread;
import lombok.extern.slf4j.Slf4j;

import java.io.IOException;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.SocketException;
import java.nio.channels.SelectionKey;
import java.nio.channels.Selector;
import java.nio.channels.ServerSocketChannel;
import java.nio.channels.SocketChannel;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.LinkedBlockingQueue;

/**
 * Created by hezhengkui on 2019/7/15.
 */
@Slf4j
public class NioServerFactory {
        protected final Set&lt;NIOServerCnxn&gt; cnxns = Collections.newSetFromMap(new ConcurrentHashMap&lt;NIOServerCnxn, Boolean&gt;());
        protected int maxClientCnxns = 60;
        private volatile boolean stopped = true;
        private AcceptThread acceptThread;
        private ConnectionExpirerThread expirerThread;
        private final ConcurrentHashMap&lt;InetAddress, Set&lt;NIOServerCnxn&gt;&gt; ipMap = new ConcurrentHashMap&lt;InetAddress, Set&lt;NIOServerCnxn&gt;&gt;();
        private ExpiryQueue&lt;NIOServerCnxn&gt; cnxnExpiryQueue;
        private final Set&lt;SelectorThread&gt; selectorThreads = new HashSet&lt;SelectorThread&gt;();
        protected WorkerService workerPool;
        final ConcurrentHashMap&lt;Long, NIOServerCnxn&gt; sessionMap = new ConcurrentHashMap&lt;Long, NIOServerCnxn&gt;();
        private int numSelectorThreads;
        private int numWorkerThreads;
        private long workerShutdownTimeoutMS;
        public static final String ZOOKEEPER_NIO_NUM_SELECTOR_THREADS = &quot;zookeeper.nio.numSelectorThreads&quot;;
        public static final String ZOOKEEPER_NIO_NUM_WORKER_THREADS = &quot;zookeeper.nio.numWorkerThreads&quot;;
        /**
         * Default worker pool shutdown timeout in ms: 5000 (5s)
         */
        public static final String ZOOKEEPER_NIO_SHUTDOWN_TIMEOUT = &quot;zookeeper.nio.shutdownTimeout&quot;;

    static {
        try {
            Selector.open().close();
        } catch (IOException ie) {
            log.error(&quot;Selector failed to open&quot;, ie);
        }
    }

    private abstract class AbstractSelectThread extends ZooKeeperThread {
        protected final Selector selector;

        public AbstractSelectThread(String name) throws IOException {
            super(name);
            // Allows the JVM to shutdown even if this thread is still running.
            setDaemon(true);
            this.selector = Selector.open();
        }

        public void wakeupSelector() {
            selector.wakeup();
        }

        /**
         * Close the selector. This should be called when the thread is about to
         * exit and no operation is going to be performed on the Selector or
         * SelectionKey
         */
        protected void closeSelector() {
            try {
                selector.close();
            } catch (IOException e) {
                log.warn(&quot;ignored exception during selector close &quot;
                        + e.getMessage());
            }
        }

        protected void cleanupSelectionKey(SelectionKey key) {
            if (key != null) {
                try {
                    key.cancel();
                } catch (Exception ex) {
                    if (log.isDebugEnabled()) {
                        log.debug(&quot;ignoring exception during selectionkey cancel&quot;, ex);
                    }
                }
            }
        }

        protected void fastCloseSock(SocketChannel sc) {
            if (sc != null) {
                try {
                    // Hard close immediately, discarding buffers
                    sc.socket().setSoLinger(true, 0);
                } catch (SocketException e) {
                    log.warn(&quot;Unable to set socket linger to 0, socket close&quot;
                            + &quot; may stall in CLOSE_WAIT&quot;, e);
                }
                closeSock(sc);
            }
        }
    }


    public static void closeSock(SocketChannel sock) {
        if (sock.isOpen() == false) {
            return;
        }

        try {
            /*
             * The following sequence of code is stupid! You would think that
             * only sock.close() is needed, but alas, it doesn&apos;t work that way.
             * If you just do sock.close() there are cases where the socket
             * doesn&apos;t actually close...
             */
            sock.socket().shutdownOutput();
        } catch (IOException e) {
            // This is a relatively common exception that we can&apos;t avoid
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during output shutdown&quot;, e);
            }
        }
        try {
            sock.socket().shutdownInput();
        } catch (IOException e) {
            // This is a relatively common exception that we can&apos;t avoid
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during input shutdown&quot;, e);
            }
        }
        try {
            sock.socket().close();
        } catch (IOException e) {
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during socket close&quot;, e);
            }
        }
        try {
            sock.close();
        } catch (IOException e) {
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during socketchannel close&quot;, e);
            }
        }
    }


    private class AcceptThread extends AbstractSelectThread {
        private final ServerSocketChannel acceptSocket;
        private final SelectionKey acceptKey;
        private final Collection&lt;SelectorThread&gt; selectorThreads;
        private Iterator&lt;SelectorThread&gt; selectorIterator;
        private volatile boolean reconfiguring = false;

        public AcceptThread(ServerSocketChannel ss, InetSocketAddress addr,
                            Set&lt;SelectorThread&gt; selectorThreads) throws IOException {
            super(&quot;NIOServerCxnFactory.AcceptThread:&quot; + addr);
            this.acceptSocket = ss;
            this.acceptKey = acceptSocket.register(selector, SelectionKey.OP_ACCEPT);
            this.selectorThreads = Collections.unmodifiableList(new ArrayList&lt;SelectorThread&gt;(selectorThreads));
            selectorIterator = this.selectorThreads.iterator();
        }

        public void run() {
            try {
                while (!stopped &amp;&amp; !acceptSocket.socket().isClosed()) {
                    try {
                        select();
                    } catch (RuntimeException e) {
                        log.warn(&quot;Ignoring unexpected runtime exception&quot;, e);
                    } catch (Exception e) {
                        log.warn(&quot;Ignoring unexpected exception&quot;, e);
                    }
                }
            } finally {
                closeSelector();
                // This will wake up the selector threads, and tell the
                // worker thread pool to begin shutdown.
                if (!reconfiguring) {
                    NioServerFactory.this.stop();
                }
                log.info(&quot;accept thread exitted run method&quot;);
            }
        }

        public void setReconfiguring() {
            reconfiguring = true;
        }

        private void select() {
            try {
                int select = selector.select();

                Iterator&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys().iterator();
                while (!stopped &amp;&amp; selectedKeys.hasNext()) {
                    SelectionKey key = selectedKeys.next();
                    selectedKeys.remove();

                    if (!key.isValid()) {
                        continue;
                    }
                    if (key.isAcceptable()) {
                        if (!doAccept()) {
                            // If unable to pull a new connection off the accept
                            // queue, pause accepting to give us time to free
                            // up file descriptors and so the accept thread
                            // doesn&apos;t spin in a tight loop.
                            pauseAccept(10);
                        }
                    } else {
                        log.warn(&quot;Unexpected ops in accept select &quot;
                                + key.readyOps());
                    }
                }
            } catch (IOException e) {
                log.warn(&quot;Ignoring IOException while selecting&quot;, e);
            }
        }

        /**
         * Mask off the listen socket interest ops and use select() to sleep
         * so that other threads can wake us up by calling wakeup() on the
         * selector.
         */
        private void pauseAccept(long millisecs) {
            acceptKey.interestOps(0);
            try {
                selector.select(millisecs);
            } catch (IOException e) {
                // ignore
            } finally {
                acceptKey.interestOps(SelectionKey.OP_ACCEPT);
            }
        }

        /**
         * Accept new socket connections. Enforces maximum number of connections
         * per client IP address. Round-robin assigns to selector thread for
         * handling. Returns whether pulled a connection off the accept queue
         * or not. If encounters an error attempts to fast close the socket.
         *
         * @return whether was able to accept a connection or not
         */
        private boolean doAccept() {
            boolean accepted = false;
            SocketChannel sc = null;
            try {
                sc = acceptSocket.accept();
                accepted = true;
                InetAddress ia = sc.socket().getInetAddress();
                int cnxncount = getClientCnxnCount(ia);

                if (maxClientCnxns &gt; 0 &amp;&amp; cnxncount &gt;= maxClientCnxns) {
                    throw new IOException(&quot;Too many connections from &quot; + ia
                            + &quot; - max is &quot; + maxClientCnxns);
                }

                log.debug(&quot;Accepted socket connection from &quot;
                        + sc.socket().getRemoteSocketAddress());
                sc.configureBlocking(false);

                // Round-robin assign this connection to a selector thread
                if (!selectorIterator.hasNext()) {
                    selectorIterator = selectorThreads.iterator();
                }
                SelectorThread selectorThread = selectorIterator.next();
                //交给selector 线程执行
                if (!selectorThread.addAcceptedConnection(sc)) {
                    throw new IOException(
                            &quot;Unable to add connection to selector queue&quot;
                                    + (stopped ? &quot; (shutdown in progress)&quot; : &quot;&quot;));
                }
            } catch (IOException e) {
                e.printStackTrace();
                // accept, maxClientCnxns, configureBlocking
                fastCloseSock(sc);
            }
            return accepted;
        }
    }

    ServerSocketChannel ss;

    public void stop() {
        stopped = true;

        // Stop queuing connection attempts
        try {
            ss.close();
        } catch (IOException e) {
            log.warn(&quot;Error closing listen socket&quot;, e);
        }

        if (acceptThread != null) {
            if (acceptThread.isAlive()) {
                acceptThread.wakeupSelector();
            } else {
                acceptThread.closeSelector();
            }
        }
        if (expirerThread != null) {
            expirerThread.interrupt();
        }
        for (SelectorThread thread : selectorThreads) {
            if (thread.isAlive()) {
                thread.wakeupSelector();
            } else {
                thread.closeSelector();
            }
        }
        if (workerPool != null) {
            workerPool.stop();
        }
    }


    class SelectorThread extends AbstractSelectThread {
        private final int id;
        private final Queue&lt;SocketChannel&gt; acceptedQueue;
        private final Queue&lt;SelectionKey&gt; updateQueue;

        public SelectorThread(int id) throws IOException {
            super(&quot;NIOServerCxnFactory.SelectorThread-&quot; + id);
            this.id = id;
            acceptedQueue = new LinkedBlockingQueue&lt;SocketChannel&gt;();
            updateQueue = new LinkedBlockingQueue&lt;SelectionKey&gt;();
        }

        /**
         * Place new accepted connection onto a queue for adding. Do this
         * so only the selector thread modifies what keys are registered
         * with the selector.
         */
        public boolean addAcceptedConnection(SocketChannel accepted) {
            if (stopped || !acceptedQueue.offer(accepted)) {
                return false;
            }
            wakeupSelector();
            return true;
        }

        /**
         * Place interest op update requests onto a queue so that only the
         * selector thread modifies interest ops, because interest ops
         * reads/sets are potentially blocking operations if other select
         * operations are happening.
         */
        public boolean addInterestOpsUpdateRequest(SelectionKey sk) {
            if (stopped || !updateQueue.offer(sk)) {
                return false;
            }
            wakeupSelector();
            return true;
        }

        /**
         * The main loop for the thread selects() on the connections and
         * dispatches ready I/O work requests, then registers all pending
         * newly accepted connections and updates any interest ops on the
         * queue.
         */
        public void run() {
            try {
                while (!stopped) {
                    try {
                        //accept事件第一次select()方法不会有任何事件触发，通过seletor.wakeUp（）唤起，
                        select();
                        //这个方法会触发registor(SelectorKey.OP_READ)事件
                        processAcceptedConnections();
                        processInterestOpsUpdateRequests();
                    } catch (RuntimeException e) {
                        log.warn(&quot;Ignoring unexpected runtime exception&quot;, e);
                    } catch (Exception e) {
                        log.warn(&quot;Ignoring unexpected exception&quot;, e);
                    }
                }

                // Close connections still pending on the selector. Any others
                // with in-flight work, let drain out of the work queue.
                for (SelectionKey key : selector.keys()) {
                    NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
                    if (cnxn.isSelectable()) {
                        cnxn.close(NIOServerCnxn.DisconnectReason.SERVER_SHUTDOWN);
                    }
                    cleanupSelectionKey(key);
                }
                SocketChannel accepted;
                while ((accepted = acceptedQueue.poll()) != null) {
                    fastCloseSock(accepted);
                }
                updateQueue.clear();
            } finally {
                closeSelector();
                // This will wake up the accept thread and the other selector
                // threads, and tell the worker thread pool to begin shutdown.
                NioServerFactory.this.stop();
                log.info(&quot;selector thread exitted run method&quot;);
            }
        }

        private void select() {
            try {
                //selector.wakeUp()方法会唤起select()方法执行
                //registor()方法同样能唤起select()方法执行
                int select = selector.select();

                Set&lt;SelectionKey&gt; selected = selector.selectedKeys();
                ArrayList&lt;SelectionKey&gt; selectedList = new ArrayList&lt;SelectionKey&gt;(selected);
                Collections.shuffle(selectedList);
                Iterator&lt;SelectionKey&gt; selectedKeys = selectedList.iterator();
                while (!stopped &amp;&amp; selectedKeys.hasNext()) {
                    SelectionKey key = selectedKeys.next();
                    selected.remove(key);

                    if (!key.isValid()) {
                        cleanupSelectionKey(key);
                        continue;
                    }
                    if (key.isReadable() || key.isWritable()) {
                        handleIO(key);
                    } else {
                        log.warn(&quot;Unexpected ops in select &quot; + key.readyOps());
                    }
                }
            } catch (IOException e) {
                log.warn(&quot;Ignoring IOException while selecting&quot;, e);
            }
        }

        /**
         * Schedule I/O for processing on the connection associated with
         * the given SelectionKey. If a worker thread pool is not being used,
         * I/O is run directly by this thread.
         */
        private void handleIO(SelectionKey key) {
            IOWorkRequest workRequest = new IOWorkRequest(this, key);
            NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();

            // Stop selecting this key while processing on its
            // connection
            cnxn.disableSelectable();
            key.interestOps(0);
            touchCnxn(cnxn);
            workerPool.schedule(workRequest);
        }

        /**
         * Iterate over the queue of accepted connections that have been
         * assigned to this thread but not yet placed on the selector.
         */
        private void processAcceptedConnections() {
            SocketChannel accepted;
            while (!stopped &amp;&amp; (accepted = acceptedQueue.poll()) != null) {
                SelectionKey key = null;
                try {
                    key = accepted.register(selector, SelectionKey.OP_READ);
                    NIOServerCnxn cnxn = createConnection(accepted, key, this);
                    key.attach(cnxn);
                    addCnxn(cnxn);
                } catch (IOException e) {
                    e.printStackTrace();
                    // register, createConnection
                    cleanupSelectionKey(key);
                    fastCloseSock(accepted);
                }
            }
        }

        /**
         * Iterate over the queue of connections ready to resume selection,
         * and restore their interest ops selection mask.
         */
        private void processInterestOpsUpdateRequests() {
            SelectionKey key;
            while (!stopped &amp;&amp; (key = updateQueue.poll()) != null) {
                if (!key.isValid()) {
                    cleanupSelectionKey(key);
                }
                NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
                if (cnxn.isSelectable()) {
                    key.interestOps(cnxn.getInterestOps());
                }
            }
        }
    }


    private void addCnxn(NIOServerCnxn cnxn) throws IOException {
        InetAddress addr = cnxn.getSocketAddress();
        if (addr == null) {
            throw new IOException(&quot;Socket of &quot; + cnxn + &quot; has been closed&quot;);
        }
        Set&lt;NIOServerCnxn&gt; set = ipMap.get(addr);
        if (set == null) {
            // in general we will see 1 connection from each
            // host, setting the initial cap to 2 allows us
            // to minimize mem usage in the common case
            // of 1 entry --  we need to set the initial cap
            // to 2 to avoid rehash when the first entry is added
            // Construct a ConcurrentHashSet using a ConcurrentHashMap
            set = Collections.newSetFromMap(
                    new ConcurrentHashMap&lt;NIOServerCnxn, Boolean&gt;(2));
            // Put the new set in the map, but only if another thread
            // hasn&apos;t beaten us to it
            Set&lt;NIOServerCnxn&gt; existingSet = ipMap.putIfAbsent(addr, set);
            if (existingSet != null) {
                set = existingSet;
            }
        }
        set.add(cnxn);

        cnxns.add(cnxn);
        touchCnxn(cnxn);
    }

    public void touchCnxn(NIOServerCnxn cnxn) {
        cnxnExpiryQueue.update(cnxn, cnxn.getSessionTimeout());
    }

    public boolean removeCnxn(NIOServerCnxn cnxn) {
        // If the connection is not in the master list it&apos;s already been closed
        if (!cnxns.remove(cnxn)) {
            return false;
        }
        cnxnExpiryQueue.remove(cnxn);

        removeCnxnFromSessionMap(cnxn);

        InetAddress addr = cnxn.getSocketAddress();
        if (addr != null) {
            Set&lt;NIOServerCnxn&gt; set = ipMap.get(addr);
            if (set != null) {
                set.remove(cnxn);
                // Note that we make no effort here to remove empty mappings
                // from ipMap.
            }
        }

        return true;
    }

    protected NIOServerCnxn createConnection(SocketChannel sock,
                                             SelectionKey sk, SelectorThread selectorThread) throws IOException {
        return new NIOServerCnxn(sock, sk, this, selectorThread);
    }

    public void removeCnxnFromSessionMap(NIOServerCnxn cnxn) {
        long sessionId = cnxn.getSessionId();
        if (sessionId != 0) {
            sessionMap.remove(sessionId);
        }
    }

    private class ConnectionExpirerThread extends ZooKeeperThread {
        ConnectionExpirerThread() {
            super(&quot;ConnnectionExpirer&quot;);
        }

        public void run() {
            try {
                while (!stopped) {
                    long waitTime = cnxnExpiryQueue.getWaitTime();
                    if (waitTime &gt; 0) {
                        Thread.sleep(waitTime);
                        continue;
                    }
                    for (NIOServerCnxn conn : cnxnExpiryQueue.poll()) {
//                        ServerMetrics.getMetrics().SESSIONLESS_CONNECTIONS_EXPIRED.add(1);
                        conn.close(NIOServerCnxn.DisconnectReason.CONNECTION_EXPIRED);
                    }
                }

            } catch (InterruptedException e) {
                log.info(&quot;ConnnectionExpirerThread interrupted&quot;);
            }
        }
    }

    private int getClientCnxnCount(InetAddress cl) {
        Set&lt;NIOServerCnxn&gt; s = ipMap.get(cl);
        if (s == null) return 0;
        return s.size();
    }


    private class IOWorkRequest extends WorkerService.WorkRequest {
        private final SelectorThread selectorThread;
        private final SelectionKey key;
        private final NIOServerCnxn cnxn;

        IOWorkRequest(SelectorThread selectorThread, SelectionKey key) {
            this.selectorThread = selectorThread;
            this.key = key;
            this.cnxn = (NIOServerCnxn) key.attachment();
        }

        public void doWork() throws InterruptedException {
            if (!key.isValid()) {
                selectorThread.cleanupSelectionKey(key);
                return;
            }

            if (key.isReadable() || key.isWritable()) {
                cnxn.doIO(key);

                // Check if we shutdown or doIO() closed this connection
                if (stopped) {
                    cnxn.close(NIOServerCnxn.DisconnectReason.SERVER_SHUTDOWN);
                    return;
                }
                if (!key.isValid()) {
                    selectorThread.cleanupSelectionKey(key);
                    return;
                }
                touchCnxn(cnxn);
            }

            // Mark this connection as once again ready for selection
            cnxn.enableSelectable();
            // Push an update request on the queue to resume selecting
            // on the current set of interest ops, which may have changed
            // as a result of the I/O operations we just performed.
            if (!selectorThread.addInterestOpsUpdateRequest(key)) {
                cnxn.close(NIOServerCnxn.DisconnectReason.CONNECTION_MODE_CHANGED);
            }
        }

        @Override
        public void cleanup() {
            cnxn.close(NIOServerCnxn.DisconnectReason.CLEAN_UP);
        }
    }

    public void configure(InetSocketAddress addr, int maxcc) throws IOException {

        maxClientCnxns = maxcc;
        cnxnExpiryQueue =
                new ExpiryQueue&lt;NIOServerCnxn&gt;(10000);
        expirerThread = new ConnectionExpirerThread();

        int numCores = Runtime.getRuntime().availableProcessors();
        // 32 cores sweet spot seems to be 4 selector threads
        numSelectorThreads = Integer.getInteger(
                ZOOKEEPER_NIO_NUM_SELECTOR_THREADS,
                Math.max((int) Math.sqrt((float) numCores / 2), 1));
        if (numSelectorThreads &lt; 1) {
            throw new IOException(&quot;numSelectorThreads must be at least 1&quot;);
        }

        numWorkerThreads = Integer.getInteger(
                ZOOKEEPER_NIO_NUM_WORKER_THREADS, 2 * numCores);
        workerShutdownTimeoutMS = Long.getLong(
                ZOOKEEPER_NIO_SHUTDOWN_TIMEOUT, 5000);

        for (int i = 0; i &lt; numSelectorThreads; ++i) {
            selectorThreads.add(new SelectorThread(i));
        }

        this.ss = ServerSocketChannel.open();
        ss.socket().setReuseAddress(true);
        log.info(&quot;binding to port &quot; + addr);
        ss.socket().bind(addr);

        ss.configureBlocking(false);
        acceptThread = new AcceptThread(ss, addr, selectorThreads);
    }

    public void start() {
        stopped = false;
        if (workerPool == null) {
            workerPool = new WorkerService(
                    &quot;NIOWorker&quot;, numWorkerThreads, false);
        }
        for (SelectorThread thread : selectorThreads) {
            if (thread.getState() == Thread.State.NEW) {
                thread.start();
            }
        }
        // ensure thread is started once and only once
        if (acceptThread.getState() == Thread.State.NEW) {
            acceptThread.start();
        }
        if (expirerThread.getState() == Thread.State.NEW) {
            expirerThread.start();
        }
    }

    public static void main(String[] args) throws IOException {
        NioServerFactory factory = new NioServerFactory();
        InetSocketAddress address = new InetSocketAddress(9999);
        factory.configure(address, 3);
        factory.start();
    }
}</code></pre><hr>
<pre><code>package server;

import lombok.extern.slf4j.Slf4j;

import java.io.IOException;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.CancelledKeyException;
import java.nio.channels.SelectionKey;
import java.nio.channels.SocketChannel;
import java.util.Queue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Created by hezhengkui on 2019/7/15.
 */
@Slf4j
public class NIOServerCnxn {
    private final SelectionKey sk;
    private final SocketChannel sock;
    private final NioServerFactory.SelectorThread selectorThread;
    private final NioServerFactory factory;
    private final AtomicBoolean selectable = new AtomicBoolean(true);
    private volatile boolean stale = false;
    private long sessionId;
    protected DisconnectReason disconnectReason = DisconnectReason.UNKNOWN;

    private int sessionTimeout = 100000000;

    private final AtomicBoolean throttled = new AtomicBoolean(false);

    private final Queue&lt;ByteBuffer&gt; outgoingBuffers =
            new LinkedBlockingQueue&lt;ByteBuffer&gt;();

    public NIOServerCnxn(SocketChannel sock, SelectionKey sk, NioServerFactory factory,
                         NioServerFactory.SelectorThread selectorThread) throws IOException {
        this.sock = sock;
        this.sk = sk;
        this.factory = factory;
        this.selectorThread = selectorThread;
        sock.socket().setTcpNoDelay(true);
        sock.socket().setSoLinger(false, -1);
        InetAddress addr = ((InetSocketAddress) sock.socket().getRemoteSocketAddress()).getAddress();
    }

    public void enableSelectable() {
        selectable.set(true);
    }

    public boolean isSelectable() {
        return sk.isValid() &amp;&amp; selectable.get();
    }

    public void close(DisconnectReason reason) {
        disconnectReason = reason;
        close();
    }

    public InetAddress getSocketAddress() {
        if (sock.isOpen() == false) {
            return null;
        }
        return sock.socket().getInetAddress();
    }

    public int getInterestOps() {
        if (!isSelectable()) {
            return 0;
        }
        int interestOps = 0;
        if (getReadInterest()) {
            interestOps |= SelectionKey.OP_READ;
        }
        if (getWriteInterest()) {
            interestOps |= SelectionKey.OP_WRITE;
        }
        return interestOps;
    }

    private boolean getWriteInterest() {
        return !outgoingBuffers.isEmpty();
    }

    private boolean getReadInterest() {
        return !throttled.get();
    }

    public void setStale() {
        stale = true;
    }

    public long getSessionId() {
        return sessionId;
    }

    private void close() {
        setStale();
        if (!factory.removeCnxn(this)) {
            return;
        }

        if (sk != null) {
            try {
                // need to cancel this selection key from the selector
                sk.cancel();
            } catch (Exception e) {
                if (log.isDebugEnabled()) {
                    log.debug(&quot;ignoring exception during selectionkey cancel&quot;, e);
                }
            }
        }

        closeSock();
    }

    public void disableSelectable() {
        selectable.set(false);
    }

    public int getSessionTimeout() {
        return sessionTimeout;
    }


    private void closeSock() {
        if (sock.isOpen() == false) {
            return;
        }

        log.debug(&quot;Closed socket connection for client &quot;
                + sock.socket().getRemoteSocketAddress()
                + (sessionId != 0 ?
                &quot; which had sessionid 0x&quot; + Long.toHexString(sessionId) :
                &quot; (no session established for client)&quot;));
        closeSock(sock);
    }


    public static void closeSock(SocketChannel sock) {
        if (sock.isOpen() == false) {
            return;
        }

        try {
            /*
             * The following sequence of code is stupid! You would think that
             * only sock.close() is needed, but alas, it doesn&apos;t work that way.
             * If you just do sock.close() there are cases where the socket
             * doesn&apos;t actually close...
             */
            sock.socket().shutdownOutput();
        } catch (IOException e) {
            // This is a relatively common exception that we can&apos;t avoid
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during output shutdown&quot;, e);
            }
        }
        try {
            sock.socket().shutdownInput();
        } catch (IOException e) {
            // This is a relatively common exception that we can&apos;t avoid
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during input shutdown&quot;, e);
            }
        }
        try {
            sock.socket().close();
        } catch (IOException e) {
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during socket close&quot;, e);
            }
        }
        try {
            sock.close();
        } catch (IOException e) {
            if (log.isDebugEnabled()) {
                log.debug(&quot;ignoring exception during socketchannel close&quot;, e);
            }
        }
    }

    protected boolean isSocketOpen() {
        return sock.isOpen();
    }


    /**
     * 具体的read、write事件在这里处理，进行数据包的拆分
     * @param k
     * @throws InterruptedException
     */
    void doIO(SelectionKey k) throws InterruptedException {
        try {
            if (isSocketOpen() == false) {
                log.warn(&quot;trying to do i/o on a null socket for session:0x&quot;
                        + Long.toHexString(sessionId));

                return;
            }
            if (k.isReadable()) {//只有客户端发送数据，才会触发这里方法的执行
                ByteBuffer byteBuffer = ByteBuffer.allocate(10);
                int rc = sock.read(byteBuffer);
                byteBuffer.flip();
                log.debug(&quot;------r--{}, {}, {}&quot;, k.interestOps(), rc, new String(byteBuffer.array()));
            }
            if (k.isWritable()) {
                log.debug(&quot;------w--&quot;, k.interestOps());
            }
        } catch (IOException e) {
            e.printStackTrace();
        } catch (CancelledKeyException e) {
            e.printStackTrace();
            log.warn(&quot;CancelledKeyException causing close of session 0x&quot; + Long.toHexString(sessionId));
            if (log.isDebugEnabled()) {
                log.debug(&quot;CancelledKeyException stack trace&quot;, e);
            }
            close(DisconnectReason.CANCELLED_KEY_EXCEPTION);
        }
    }

    public enum DisconnectReason {
        UNKNOWN(&quot;unknown&quot;),
        SERVER_SHUTDOWN(&quot;server_shutdown&quot;),
        CONNECTION_EXPIRED(&quot;connection_expired&quot;),
        CANCELLED_KEY_EXCEPTION(&quot;cancelled_key_exception&quot;),
        CLEAN_UP(&quot;clean_up&quot;),
        CONNECTION_MODE_CHANGED(&quot;connection_mode_changed&quot;);
        String disconnectReason;

        DisconnectReason(String reason) {
            this.disconnectReason = reason;
        }

        public String toDisconnectReasonString() {
            return disconnectReason;
        }
    }

}</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="/2019/07/15/java/java-nio/" data-id="cklaphfd20006m99bxt3vtkhx" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-redis/redis-info信息" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/12/redis/redis-info信息/" class="article-date">
  <time datetime="2019-07-12T05:20:20.000Z" itemprop="datePublished">2019-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/12/redis/redis-info信息/">redis info信息</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>INFO [section]</p>
<pre><code>以一种易于解释（parse）且易于阅读的格式，返回关于 Redis 服务器的各种信息和统计数值。

通过给定可选的参数 section ，可以让命令只返回某一部分的信息：</code></pre><p>server 部分记录了 Redis 服务器的信息，它包含以下域：</p>
<pre><code>redis_version : Redis 服务器版本
redis_git_sha1 : Git SHA1
redis_git_dirty : Git dirty flag
os : Redis 服务器的宿主操作系统
arch_bits : 架构（32 或 64 位）
multiplexing_api : Redis 所使用的事件处理机制
gcc_version : 编译 Redis 时所使用的 GCC 版本
process_id : 服务器进程的 PID
run_id : Redis 服务器的随机标识符（用于 Sentinel 和集群）
tcp_port : TCP/IP 监听端口
uptime_in_seconds : 自 Redis 服务器启动以来，经过的秒数
uptime_in_days : 自 Redis 服务器启动以来，经过的天数
lru_clock : 以分钟为单位进行自增的时钟，用于 LRU 管理
clients 部分记录了已连接客户端的信息，它包含以下域：

connected_clients : 已连接客户端的数量（不包括通过从属服务器连接的客户端）
client_longest_output_list : 当前连接的客户端当中，最长的输出列表
client_longest_input_buf : 当前连接的客户端当中，最大输入缓存
blocked_clients : 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量
memory 部分记录了服务器的内存信息，它包含以下域：

used_memory : 由 Redis 分配器分配的内存总量，以字节（byte）为单位
used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量
used_memory_rss : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。
used_memory_peak : Redis 的内存消耗峰值（以字节为单位）
used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值
used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位）
mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率
mem_allocator : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc 。
在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。
当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。
内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。
当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。
Because Redis does not have control over how its allocations are mapped to memory pages, high used_memory_rss is often the result of a spike in memory usage.

当 Redis 释放内存时，分配器可能会，也可能不会，将内存返还给操作系统。
如果 Redis 释放了内存，却没有将内存返还给操作系统，那么 used_memory 的值可能和操作系统显示的 Redis 内存占用并不一致。
查看 used_memory_peak 的值可以验证这种情况是否发生。
persistence 部分记录了跟 RDB 持久化和 AOF 持久化有关的信息，它包含以下域：

loading : 一个标志值，记录了服务器是否正在载入持久化文件。
rdb_changes_since_last_save : 距离最近一次成功创建持久化文件之后，经过了多少秒。
rdb_bgsave_in_progress : 一个标志值，记录了服务器是否正在创建 RDB 文件。
rdb_last_save_time : 最近一次成功创建 RDB 文件的 UNIX 时间戳。
rdb_last_bgsave_status : 一个标志值，记录了最近一次创建 RDB 文件的结果是成功还是失败。
rdb_last_bgsave_time_sec : 记录了最近一次创建 RDB 文件耗费的秒数。
rdb_current_bgsave_time_sec : 如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。
aof_enabled : 一个标志值，记录了 AOF 是否处于打开状态。
aof_rewrite_in_progress : 一个标志值，记录了服务器是否正在创建 AOF 文件。
aof_rewrite_scheduled : 一个标志值，记录了在 RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作。
aof_last_rewrite_time_sec : 最近一次创建 AOF 文件耗费的时长。
aof_current_rewrite_time_sec : 如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。
aof_last_bgrewrite_status : 一个标志值，记录了最近一次创建 AOF 文件的结果是成功还是失败。
如果 AOF 持久化功能处于开启状态，那么这个部分还会加上以下域：

aof_current_size : AOF 文件目前的大小。
aof_base_size : 服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小。
aof_pending_rewrite : 一个标志值，记录了是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行。
aof_buffer_length : AOF 缓冲区的大小。
aof_rewrite_buffer_length : AOF 重写缓冲区的大小。
aof_pending_bio_fsync : 后台 I/O 队列里面，等待执行的 fsync 调用数量。
aof_delayed_fsync : 被延迟的 fsync 调用数量。
stats 部分记录了一般统计信息，它包含以下域：

total_connections_received : 服务器已接受的连接请求数量。
total_commands_processed : 服务器已执行的命令数量。
instantaneous_ops_per_sec : 服务器每秒钟执行的命令数量。
rejected_connections : 因为最大客户端数量限制而被拒绝的连接请求数量。
expired_keys : 因为过期而被自动删除的数据库键数量。
evicted_keys : 因为最大内存容量限制而被驱逐（evict）的键数量。
keyspace_hits : 查找数据库键成功的次数。
keyspace_misses : 查找数据库键失败的次数。
pubsub_channels : 目前被订阅的频道数量。
pubsub_patterns : 目前被订阅的模式数量。
latest_fork_usec : 最近一次 fork() 操作耗费的毫秒数。
replication : 主/从复制信息

role : 如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器。
如果当前服务器是一个从服务器的话，那么这个部分还会加上以下域：

master_host : 主服务器的 IP 地址。
master_port : 主服务器的 TCP 监听端口号。
master_link_status : 复制连接当前的状态， up 表示连接正常， down 表示连接断开。
master_last_io_seconds_ago : 距离最近一次与主服务器进行通信已经过去了多少秒钟。
master_sync_in_progress : 一个标志值，记录了主服务器是否正在与这个从服务器进行同步。
如果同步操作正在进行，那么这个部分还会加上以下域：

master_sync_left_bytes : 距离同步完成还缺少多少字节数据。
master_sync_last_io_seconds_ago : 距离最近一次因为 SYNC 操作而进行 I/O 已经过去了多少秒。
如果主从服务器之间的连接处于断线状态，那么这个部分还会加上以下域：

master_link_down_since_seconds : 主从服务器连接断开了多少秒。
以下是一些总会出现的域：

connected_slaves : 已连接的从服务器数量。
对于每个从服务器，都会添加以下一行信息：

slaveXXX : ID、IP 地址、端口号、连接状态
cpu 部分记录了 CPU 的计算量统计信息，它包含以下域：

used_cpu_sys : Redis 服务器耗费的系统 CPU 。
used_cpu_user : Redis 服务器耗费的用户 CPU 。
used_cpu_sys_children : 后台进程耗费的系统 CPU 。
used_cpu_user_children : 后台进程耗费的用户 CPU 。
commandstats 部分记录了各种不同类型的命令的执行统计信息，比如命令执行的次数、命令耗费的 CPU 时间、执行每个命令耗费的平均 CPU 时间等等。对于每种类型的命令，这个部分都会添加一行以下格式的信息：

cmdstat_XXX:calls=XXX,usec=XXX,usecpercall=XXX
cluster 部分记录了和集群有关的信息，它包含以下域：

cluster_enabled : 一个标志值，记录集群功能是否已经开启。
keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库已经被删除的过期键数量等。对于每个数据库，这个部分都会添加一行以下格式的信息：

dbXXX:keys=XXX,expires=XXX
除上面给出的这些值以外， section 参数的值还可以是下面这两个：

all : 返回所有信息
default : 返回默认选择的信息
当不带参数直接调用 INFO 命令时，使用 default 作为默认参数。

不同版本的 Redis 可能对返回的一些域进行了增加或删减。

因此，一个健壮的客户端程序在对 INFO 命令的输出进行分析时，应该能够跳过不认识的域，并且妥善地处理丢失不见的域。

可用版本：&gt;= 1.0.0时间复杂度：O(1)返回值：具体请参见下面的测试代码。</code></pre><p>redis&gt; INFO</p>
<pre><code># Server
redis_version:2.9.11
redis_git_sha1:937384d0
redis_git_dirty:0
redis_build_id:8e9509442863f22
redis_mode:standalone
os:Linux 3.13.0-35-generic x86_64
arch_bits:64
multiplexing_api:epoll
gcc_version:4.8.2
process_id:4716
run_id:26186aac3f2380aaee9eef21cc50aecd542d97dc
tcp_port:6379
uptime_in_seconds:362
uptime_in_days:0
hz:10
lru_clock:1725349
config_file:

# Clients
connected_clients:1
client_longest_output_list:0
client_biggest_input_buf:0
blocked_clients:0

# Memory
used_memory:508536
used_memory_human:496.62K
used_memory_rss:7974912
used_memory_peak:508536
used_memory_peak_human:496.62K
used_memory_lua:33792
mem_fragmentation_ratio:15.68
mem_allocator:jemalloc-3.2.0

# Persistence
loading:0
rdb_changes_since_last_save:6
rdb_bgsave_in_progress:0
rdb_last_save_time:1411011131
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:-1
rdb_current_bgsave_time_sec:-1
aof_enabled:0
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok

# Stats
total_connections_received:2
total_commands_processed:4
instantaneous_ops_per_sec:0
rejected_connections:0
sync_full:0
sync_partial_ok:0
sync_partial_err:0
expired_keys:0
evicted_keys:0
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:0
migrate_cached_sockets:0

# Replication
role:master
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0

# CPU
used_cpu_sys:0.21
used_cpu_user:0.17
used_cpu_sys_children:0.00
used_cpu_user_children:0.00

# Cluster
cluster_enabled:0

# Keyspace
db0:keys=2,expires=0,avg_ttl=0</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="/2019/07/12/redis/redis-info信息/" data-id="cklaphfd6000cm99bfidwb4st" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/redis/">redis</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-linux-init" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/12/linux-init/" class="article-date">
  <time datetime="2019-07-12T04:28:35.000Z" itemprop="datePublished">2019-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/12/linux-init/">linux 命令</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>linux 高效命令</p>
<pre><code>vmstat -n 5 | awk &apos;{print strftime(&quot;[%Y-%m-%d %H:%M:%S]&quot;),$0}&apos;</code></pre><p>在maxpro上添加访问minikube内部ip的路由</p>
<pre><code>sudo route -n add 172.17.0.0/24 192.168.99.100
sudo route -n delete 172.17.0.0/24 192.168.99.100</code></pre><p>解压命令</p>
<pre><code>gunzip -c make-4.0.tar.gz |tar xvf -
/lib/systemd/system/docker.service --insecure-registry 192.168.99.1:5000</code></pre><p>设置centos时区</p>
<pre><code>#timedatectl list-timezones  # 列出所有时区
#timedatectl set-local-rtc 1  
# 将硬件时钟调整为与本地时钟一致, 0 为设置为 UTC 时间
#timedatectl set-timezone Asia/Shanghai  
# 设置系统时区为上海
#cp/usr/share/zoneinfo/Asia/Shanghai /etc/localtime</code></pre><p>curl使用</p>
<pre><code>curl -X POST http://localhost:8080/upload \
  -F &quot;file=@/Users/appleboy/test.zip&quot; \
  -H &quot;Content-Type: multipart/form-data”

curl -o /dev/null -w %{time_namelookup}::%{time_connect}::%{time_starttransfer}::%{time_total}::%{speed_download}&quot;\n&quot; &quot;http://sa.a.alldk.com/“

echo &quot;select adx,adid,adslotid,count(1) as total from common_ad_req_all where logday = &apos;2019-05-27&apos; and hour = &apos;14&apos; group by adx,adid,adslotid;&quot;| curl dcs-inc.xxx.com:8123/\?database=common_ad\&amp;user=&apos;&apos;\&amp;password=&apos;&apos; --data-binary @-</code></pre><p>mysql explain执行计划</p>
<pre><code>explain
select distinct (ng.id), ng.`name`
from naming_group ng left JOIN naming_group_user  ngu on ng.id = ngu.group_id where ngu.account_name = &apos;hezhengkui&apos;
AND ng.name LIKE CONCAT(&apos;%g%&apos;)



explain
select ng.id, ng.`name`
from naming_group ng left JOIN naming_group_user  ngu on ng.id = ngu.group_id where ngu.account_name = &apos;hezhengkui&apos;
AND ng.name LIKE CONCAT(&apos;%g%&apos;)



explain
select g.id,g.name from naming_group as g where 1=1 and exists(
    select 1 from naming_group_user as u where 1 = 1 and g.id = u.group_id and u.account_name = &apos;hezhengkui&apos; and g.name like concat(&apos;gg%&apos;)
)



explain
select g.id,g.name from naming_group as g where 1=1 and exists(
    select 1 from naming_group_user as u where 1 = 1 and g.id = u.group_id and u.account_name = &apos;hezhengkui&apos; 
)
AND G.name LIKE CONCAT(&apos;%gg%&apos;)</code></pre><p>Linux内核参数优化</p>
<pre><code>#!/bin/bash
#adwpc

OBJ=&quot;&quot;
CURDIR=$(cd `dirname $0`; pwd)
NAME=`basename $0`
LOG=&quot;$CURDIR/$NAME.log&quot;
ERR=&quot;$CURDIR/$NAME.err&quot;



function sysctl_conf_optm() {
    sudo mv /etc/sysctl.conf /etc/sysctl.conf.`date +%Y%m%d%H%M%S`
    cat &gt;&gt; sysctl.conf &lt;&lt; EOF
#the process open file limit
fs.nr_open = 1048576
#the system open file limit
fs.file-max = 1048576
#port range
net.ipv4.ip_local_port_range = 1024 65535
# Decrease the time default value for connections to keep alive
#tcp keepalive packet cycle
net.ipv4.tcp_keepalive_time = 600
#tcp keepalive packet retry times
net.ipv4.tcp_keepalive_probes = 10
#tcp keepalive packet retry delay when packet lost
net.ipv4.tcp_keepalive_intvl = 30
# TCP window scaling for high-throughput, high-pingtime TCP performance
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 65535
# Controls the System Request debugging functionality of the kernel
#disable debug
kernel.sysrq = 0
# Don&apos;t ignore directed pings
net.ipv4.icmp_echo_ignore_all = 0
# Controls the maximum size of a message queue, in bytes
kernel.msgmnb = 65536
# Controls the maximum msg num of queue
kernel.msgmni = 1024
# Controls the default maximum size of a message
kernel.msgmax = 655360
# specifies the minimum virtual address that a process is allowed to mmap
vm.mmap_min_addr = 4096
# How many times to retry killing an alive TCP connection
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_retries1 = 3
# Increase the maximum memory used to reassemble IP fragments
net.ipv4.ipfrag_high_thresh = 512000
net.ipv4.ipfrag_low_thresh = 446464
# Set maximum amount of memory allocated to shm to 16G
kernel.shmmax = 16000000000
kernel.shmall = 4000000
# number of unprocessed input packets before kernel starts dropping them
net.core.netdev_max_backlog = 262144
net.core.rmem_default = 1048576
net.core.optmem_max = 1048576
net.core.rmem_max = 33554432
net.core.somaxconn = 65535
net.core.wmem_max = 1048576
#iptables
net.netfilter.nf_conntrack_max = 1048576
net.ipv4.tcp_fin_timeout = 2
net.ipv4.tcp_max_orphans = 262144
net.ipv4.tcp_timestamps = 1
# 
net.ipv4.tcp_rmem = 32768 131072 16777216
net.ipv4.tcp_wmem = 8192 131072 16777216
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_tw_buckets = 10000
# Do a &apos;modprobe tcp_cubic&apos; first
net.ipv4.tcp_congestion_control = cubic
# cache ssthresh from previous connection
net.ipv4.tcp_no_metrics_save = 0
net.ipv4.tcp_moderate_rcvbuf = 0
# Enable a fix for RFC1337 - time-wait assassination hazards in TCP
net.ipv4.tcp_rfc1337 = 1
# UDP parameters
net.ipv4.udp_mem = 262144 1048576 10485760
net.ipv4.udp_rmem_min = 1048576
net.ipv4.udp_wmem_min = 1048576
# Enable ignoring broadcasts request
net.ipv4.icmp_echo_ignore_broadcasts = 1
# Enable bad error message Protection
net.ipv4.icmp_ignore_bogus_error_responses = 1
vm/min_free_kbytes = 65536
# disable ipv6
# net.ipv6.conf.all.disable_ipv6 = 1
# net.ipv6.conf.default.disable_ipv6 = 1
# net.ipv6.conf.lo.disable_ipv6 = 1
# This will ensure that immediately subsequent connections use the new values
net.ipv4.route.flush = 1
# net.ipv6.route.flush = 1
EOF
    sudo mv sysctl.conf /etc
    sudo sysctl -p

    sudo cp /etc/security/limits.conf /etc/security/limits.conf.`date +%Y%m%d%H%M%S`
    cat &gt;&gt; limits.conf &lt;&lt; EOF
#the shell open file limit, the soft limit is a warning point, and the hard limit is a block.
#relogin make params work
* soft nofile 1048575
* hard nofile 1048575
* soft nproc 1048575
* hard nproc 1048575
# End of file
EOF
    sudo mv limits.conf /etc/security

}</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="/2019/07/12/linux-init/" data-id="cklaphfbq0000m99b3um5cpsi" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linux/">linux</a></li></ul>

    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    <!--

 <div class="widget-wrap">
     
        <h3 class="follow-title ">Follow me</h3>
     
    <div class="widget follow">
      
      
      
      
            <a class="email" aria-hidden="true"  href="mailto:hezkvectory@163.com" target="_blank" title="邮箱"></a>
      
    </div>
  </div>


-->

  
    
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title tagcloud">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/http/" style="font-size: 14px;">http</a> <a href="/tags/java/" style="font-size: 25px;">java</a> <a href="/tags/k8s/" style="font-size: 14px;">k8s</a> <a href="/tags/linux/" style="font-size: 14px;">linux</a> <a href="/tags/redis/" style="font-size: 14px;">redis</a> <a href="/tags/tcp/" style="font-size: 14px;">tcp</a> <a href="/tags/zookeeper/" style="font-size: 14px;">zookeeper</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/15/java/java-nio/">java-nio-server</a>
          </li>
        
          <li>
            <a href="/2019/07/12/redis/redis-info信息/">redis info信息</a>
          </li>
        
          <li>
            <a href="/2019/07/12/linux-init/">linux 命令</a>
          </li>
        
          <li>
            <a href="/2019/07/12/http/http/">http</a>
          </li>
        
          <li>
            <a href="/2019/07/12/http/tcp/">tcp三次握手、四次挥手</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title archive">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  
    <!--微信公众号二维码-->

  <div class="widget-wrap">
    <h3 class="follow-title ">WeChat</h3>
<!--    <div class="widget wechat-widget">
        <img src="http://blog.giscafer.com/static/images/qrcode_giscafer.jpg" alt="扫码关注" width="250"/>
    </div>
 --> </div>


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2021 hezhengkui&nbsp;&nbsp;
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;hzkvectory@163.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>